{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying Kinetics Models: Part 1 - Software Infrastructure\n",
    "\n",
    "Testing provides a way to find defects in products such as cars televisions, food, and software.\n",
    "\n",
    "There are two broad objectives for testing. **Validation** determines if the product provides a useful purpose. In terms of kinetics models this means that the model is accurate, and is useful for its intended objectives. For example, to a model for predicting ICU usage for COVID-19 patients might involve: (a) checking its predictions vs. observed future data and (b) estimating the mortality, morbidity, and cost implications resulting from inaccuracies of the model. In general, validation is context and discipline specific.\n",
    "\n",
    "**Verification** is about determining if the product performs according to its specification. For kinetics models, this means that the model dynamics of the model are consistent with what is intended (even if these are not the *correct* dynamics). Validation software engineering typically takes the form of unittests, codes that detect errors in the functioning of software components.\n",
    "\n",
    "This tutorial focuses on verification of kinetics models, ensuring that the intended dynamics are produced by the model. This tutorial is divided into two parts. The first part describes the software setup for doing verification of kinetics models in Jupyter Notebooks. The second part describes an approach to writing kinetics tests using this software setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tellurium as te\n",
    "from teUtils.named_timeseries import NamedTimeseries, TIME\n",
    "from teUtils.timeseries_plotter import TimeseriesPlotter, PlotOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation and Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing is the process by which you exercise your code to determine if it performs as expected. The code you are testing is referred to as the code under test.\n",
    "\n",
    "There are two parts to writing tests.\n",
    "1. invoking the code under test so that it is exercised in a particular way;\n",
    "1. evaluating the results of executing code under test to determine if it behaved as expected.\n",
    "\n",
    "The collection of tests performed are referred to as the test cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing in a Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " simple_model = '''\n",
    "    model example1\n",
    "      S1 -> S2; k1*S1\n",
    "      S1 = 10\n",
    "      S2 = 0\n",
    "      k1 = 0.1\n",
    "    end\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation runner\n",
    "def runSimulation(model):\n",
    "    \"\"\"\n",
    "    Runs a simulation for an antimony model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    NamedTimeseries\n",
    "    \"\"\"\n",
    "    rr = te.loada(model)\n",
    "    data = rr.simulate()\n",
    "    return NamedTimeseries(named_array=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests return True if passed and False if fail.\n",
    "def test1(ts):\n",
    "    return len(ts) > 0\n",
    "\n",
    "def test2(ts):\n",
    "    s1 = ts[\"S1\"]\n",
    "    return s1[0] == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test runner\n",
    "def runTests(model):\n",
    "    ts = runSimulation(model)\n",
    "    is_ok = True\n",
    "    for test in [test1, test2]:\n",
    "        is_ok = is_ok and test(ts)\n",
    "    if is_ok:\n",
    "        print(\"OK.\")\n",
    "    else:\n",
    "        print(\"Problems encountered in model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK.\n"
     ]
    }
   ],
   "source": [
    "runTests(simple_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, you will create tests for a new model called ``model``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = '''\n",
    "        # Reactions   \n",
    "        J1: S1 -> S2; k1*S1\n",
    "        J2: S2 -> S3; k2*S2\n",
    "        J3: S3 -> S4; k3*S3\n",
    "        J4: S4 -> S5; k4*S4\n",
    "        J5: S5 -> S6; k5*S5;\n",
    "        # Species initializations     \n",
    "        k1 = 1; k2 = 2; k3 = 3; k4 = 4; k5 = 5;\n",
    "        S1 = 10; S2 = 0; S3 = 0; S4 = 0; S5 = 0; S6 = 0;\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the existing tests for this model. Did the model pass?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK.\n"
     ]
    }
   ],
   "source": [
    "runTests(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improve the tests by including print statements that describe the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "def test1(ts):\n",
    "    if len(ts) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        print(\"No data produced by simulation.\")\n",
    "        return False\n",
    "    \n",
    "def test2(ts):\n",
    "    s1 = ts[\"S1\"]\n",
    "    if s1[0] == 10:\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Initial value of S1 should be 10!\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK.\n"
     ]
    }
   ],
   "source": [
    "runTests(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix the error in the model and re-run the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix error in model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write additional tests.\n",
    "1. The maxium value of S1 is at 0.\n",
    "1. The maximum value of S6 is its last value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test3(ts):\n",
    "    \"\"\"\n",
    "    Check that the maximum value of S1 is at time 0.\n",
    "    \"\"\"\n",
    "    s1 = ts[\"S1\"]\n",
    "    if s1[0] == max(s1):\n",
    "        return True\n",
    "    else:\n",
    "        print(\"The initial value of S1 should be 0!\")\n",
    "        return False\n",
    "    \n",
    "def test4(ts):\n",
    "    \"\"\"\n",
    "    Check that the maximum value of s6 is at the end of the simulation.\n",
    "    \"\"\"\n",
    "    s6 = ts[\"S6\"]\n",
    "    if s6[-1] == max(s6):\n",
    "        return True\n",
    "    else:\n",
    "        print(\"The maximum value of S6 should be at the last time!\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newRunTests(model):\n",
    "    ts = runSimulation(model)\n",
    "    is_ok = True\n",
    "    for test in [test1, test2, test3, test4]:\n",
    "        is_ok = is_ok and test(ts)\n",
    "    if is_ok:\n",
    "        print(\"OK.\")\n",
    "    else:\n",
    "        print(\"Problems encountered in model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK.\n"
     ]
    }
   ],
   "source": [
    "newRunTests(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced\n",
    "A problem with the above infrastructre is that we need to revise the test runner every time we add or remove a test. Having to make coordinated changes in software is extremely bad practice and is a common cause of software errors.\n",
    "\n",
    "Another solution is to have a \"test container\". In python, this is done by creating a python class in which tests reside. We won't provide details about python classes here. Rather, you can just use the code below. The main changes are the ``__init__`` function at the top, the arguments to the test, and the use of ``self.ts`` to access the timeseries. The ``run`` function runs all functions in ``TestContainer`` that begin with ``test``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestContainer(object):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        ts = runSimulation(model)\n",
    "        self.ts = ts\n",
    "    \n",
    "    def test1(self):\n",
    "        if len(self.ts) > 0:\n",
    "            return True\n",
    "        else:\n",
    "            print(\"No data produced by simulation.\")\n",
    "            return False\n",
    "\n",
    "    def test2(self):\n",
    "        s1 = self.ts[\"S1\"]\n",
    "        if s1[0] == 10:\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Initial value of S1 should be 10!\")\n",
    "            return False\n",
    "        \n",
    "    def run(self):\n",
    "        is_ok = True\n",
    "        for item in dir(self):\n",
    "            if item[0:4] == \"test\":\n",
    "                # Construct the function call\n",
    "                func = \"self.%s()\" % item\n",
    "                is_ok = is_ok and eval(func)\n",
    "        if is_ok:\n",
    "            print(\"OK.\")\n",
    "        else:\n",
    "            print(\"Problems encountered in model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the tests, you do the follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK.\n"
     ]
    }
   ],
   "source": [
    "tester = TestContainer(simple_model)\n",
    "tester.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an error by modifying the initial value of S1 to show that this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, add ``test3`` and ``test4`` to ``TestContainer``. We don't have to modify any other codes to include these tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
